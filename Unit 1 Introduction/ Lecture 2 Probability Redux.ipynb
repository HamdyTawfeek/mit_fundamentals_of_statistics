{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>  <span style=\"color:blue\">Probability Redux </span> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Probability is an essential part of statistics.\n",
    "We have, on the one hand, the truth, a stochastic process,\n",
    "a data generating process, that is generating\n",
    "the observations that we see.\n",
    "And on the other hand, we have just partial observations\n",
    "of it.\n",
    "And probability tells us how to, given\n",
    "the truth, what the observations would look like.\n",
    "And statistics is reverse engineering this process,\n",
    "starting from observations, trying\n",
    "to infer what is the data generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Laws of Large Numbers and Central Limit Theorem</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A lot in statistics are averages and we replace expectation by averages. The thing that justifies that expectations are close to averages is the Law of Large Numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X,X_1,X_2,\\ldots ,X_ n$ be i.i.d random variables, with $\\mu =\\mathbb E[X]$ and $\\sigma ^2=\\textsf{Var}[X]$ \n",
    "\n",
    "<span style=\"color:green\"><b>Laws (weak and strong) of large numbers (LLN): </b> </span>\n",
    "\n",
    "$$\\bar X_ n:=\\frac{1}{n}\\sum _{i=1}^ n X_ i \\xrightarrow [n\\to \\infty ]{\\mathbf{P},\\mbox{ a.s.}} \\mu$$\n",
    "\n",
    "where the convergence is in probability (as denoted by  $\\mathbf{P}$  on the convergence arrow) and almost surely (as denoted by  a.s.  on the arrow) for the weak and strong laws respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says that if you have a large enough sample size, if n is large enough, then\n",
    "you should be close, right? That's what limit means. how close? This does not tell us anything. So we need a more quantitative way to say that, OK, maybe for a given finite sample size, what can we say? And that's what the central limit theorem tells us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\"><b>Central limit theorem (CLT):  </b> </span>\n",
    "\n",
    "$$\\displaystyle \\sqrt{n}\\, \\frac{\\bar X_ n-\\mu }{\\sigma } \\displaystyle \\xrightarrow [n\\to \\infty ]{(d)} \\displaystyle \\mathcal{N}(0,1)$$\n",
    "\n",
    "$\\displaystyle \\text {or equivalently,}$ $$\\displaystyle \\sqrt{n}\\, \\left(\\bar X_ n-\\mu \\right) \\displaystyle \\xrightarrow [n\\to \\infty ]{(d)} \\displaystyle \\mathcal{N}(0,{\\sigma ^2}) $$\n",
    "\n",
    "where the convergence is in distribution, as denoted by  $(d)$  on top of the convergence arrow. \n",
    "\n",
    "> As a rule of thumb n should be larger than or equal to 30.\n",
    "\n",
    "> The message to get from CLT is that $\\mid \\bar X_ n-\\mu \\mid \\leq \\frac{c \\sigma }{\\sqrt{n} }. $ The role of the Gaussian will allow us to be extremely precise about the constant $c$ we use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  <span style=\"color:red\">CLT Concept Check </span>\n",
    "\n",
    "Let $X_1,X_2,\\dots ,X_ n$ be a sequence of i.i.d. random variables with $\\mathbb E[X]=\\mu$ and $\\textsf{Var}(X)=\\sigma ^2$. \n",
    "\n",
    "$$\\overline{X}_ n=\\frac{X_1+X_2+\\cdots +X_ n}{n}$$\n",
    "\n",
    " For a finite  n , what is the distribution of $\\overline{X}$? <span style=\"color:red\">A Gaussian. </span>\n",
    " \n",
    "  Assuming that  n  is very large. what is the best approximate characterization of the distribution of $\\overline{X}$?\n",
    " <span style=\"color:red\">$N(\\mu ,\\sigma ^2/n)$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Hoeffding's Inequality </span></h3> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $n \\,(n>0)$ i.i.d. random variables  $X_1,X_2,\\ldots ,X_ n\\stackrel{iid}{\\sim }X$  that are almost surely **bounded** â€“ meaning  $\\mathbf{P}(X \\notin [a,b])=0$.\n",
    "\n",
    "$$\\displaystyle  \\displaystyle \\mathbf{P}\\left(\\left|\\overline{X}_ n-\\mathbb E[X]\\right|\\geq \\epsilon \\right) \\leq 2 \\exp \\left(-\\frac{2n\\epsilon ^2}{(b-a)^2}\\right)\\qquad \\text {for all }\\epsilon >0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When n is not large enough Hoeffding's Inequality is essentially delivering the same message\n",
    "that $\\mid \\bar X_ n-\\mu \\mid  $ is close to some deviation.\n",
    "* So the conclusion of `the law of large numbers` and `Hoeffding's Inequality` is the average is a good replacement for the expectation.\n",
    "* Hoeffding's inequality\n",
    "is more conservative than the central limit theorem.\n",
    "But it's also less precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Markov inequality</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random variable  $X\\geq 0$  with mean  $\\mu >0$ , and any number  $t>0$ :\n",
    "\n",
    "$$\\displaystyle  \\mathbf{P}(X\\geq t)\\leq \\frac{\\mu }{t}.$$\n",
    "\n",
    "Note that the Markov inequality is restricted to **non-negative** random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Chebyshev inequality</span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random variable  $X$  with (finite) mean  $\\mu$  and variance  $\\sigma ^2$ , and for any number  $t>0$ ,\n",
    "\n",
    "$$\\displaystyle  \\displaystyle \\mathbf{P}\\left(\\left|X-\\mu \\right|\\geq t\\right)\\leq \\frac{\\sigma ^2}{t^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: When Markov inequality is applied to  $(X-\\mu )^2$ , we obtain Chebyshev's inequality. Markov inequality is also used in the proof of Hoeffding's inequality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Gaussian Distribution </span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Distribution is ubiquitous because of the central limit theorem. It says that no matter what the distribution of your x size is, as long as it has mean and variance, their average will, properly rescaled, go to a Gaussian. $f_{N(\\mu, \\sigma^2)}(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/04gauess.png\"> </img>\n",
    "* Tails decay very fast (like $ \\exp \\left(-\\frac{x^2}{2 \\sigma^2}\\right)$ ): almost in finite interval.\n",
    "* There is no closed form for their cumulative distribution function (CDF). We use tables (or computers):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:green\"><b>Properties of the Gaussian distribution</b> </span>\n",
    "\n",
    "\n",
    "* Affine Transformation and Standardization\n",
    "\n",
    "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\rightarrow a X + b \\sim \\mathcal{N}(a  \\mu + b, a^2 \\sigma^2)$$\n",
    "\n",
    "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\rightarrow Z = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "$$P(u \\leq X \\leq v) = P(\\frac{u - \\mu}{\\sigma} \\leq Z \\leq \\frac{v - \\mu}{\\sigma})$$\n",
    "\n",
    "\n",
    "* Symmetry\n",
    "\n",
    "$$X \\sim \\mathcal{N}(0, \\sigma^2) \\rightarrow -X \\sim \\mathcal{N}(0, \\sigma^2)$$\n",
    "\n",
    "$$\\textrm{if} \\, \\,  x > 0, \\,\\, P(|X| > x) = P(X > x) + P(X < -x) =  P(X > x) + P(-X > x) = 2 P(X > x)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3><span style=\"color:#6600CC\">Quantiles </span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\alpha \\in (0, 1)$. The quantile of order $1-\\alpha$  of a variable  $X$ , denoted by  $q_{\\alpha }$  (specific to a particular  $X$ ), is the number such that $\\displaystyle \\mathbf{P}\\left(X\\leq q_{\\alpha }\\right)=1-\\alpha$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In general, the quantiles of any continous random variable satisfes  $q_{a}>q_{b}$  if  $a<b$.\n",
    "\n",
    "- $F_X(q_\\alpha) = 1 - \\alpha$\n",
    "\n",
    "- If $F_X$ is invertible, $ q_\\alpha = F_X^{-1}(1 - \\alpha) $. Ex: for normal distribution $q_{\\alpha}=\\Phi ^{-1}(1-\\alpha).$\n",
    "\n",
    "\n",
    "- $P(X > q_\\alpha) = 1 - F_X(q_\\alpha) = \\alpha$\n",
    "\n",
    "- If $X = Z \\sim \\mathcal{N}(0,1) \\rightarrow P(|X| > q_{\\alpha/2}) = \\alpha$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Modes of Convergence </span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $T_n$ is a sequence of random variables.\n",
    "- $T$ is a random variable ($T$ may be deterministic)\n",
    "\n",
    "\n",
    "<span style=\"color:green\"><b>Almost Surely (a.s) Convergence </b> </span>\n",
    "\n",
    "\n",
    "\n",
    "$$T_n \\xrightarrow[n \\to \\infty]{a.s} T  \\quad  \\textsf{iff} \\quad P(\\{\\omega : T_n(\\omega) \\xrightarrow[n \\to \\infty]{} T(\\omega)\\}) = 1$$\n",
    "\n",
    "<span style=\"color:green\"><b>Convergence in Probability</b> </span>\n",
    "\n",
    "\n",
    "\n",
    "$$T_n \\xrightarrow[n \\to \\infty]{P} T \\quad  \\textsf{iff} \\quad P(|T_n - T| \\geq \\epsilon) \\xrightarrow[n \\to \\infty]{} = 0, \\quad   \\forall \\epsilon > 0$$\n",
    "\n",
    "<span style=\"color:green\"><b>Convergence in Distribution</b> </span>\n",
    "\n",
    "\n",
    "$$T_n \\xrightarrow[n \\to \\infty]{(d)} T \\quad  \\textsf{iff} \\quad E[f(T_n)] \\xrightarrow[n \\to \\infty]{} E[f(T)]$$ \n",
    "\n",
    "\n",
    "\n",
    "for all continuous and bounded functions $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Slutsky's Theorem</b> </span>\n",
    "\n",
    "For convergence in distribution, the Slutsky's Theorem will be our main tool.\n",
    "\n",
    "Let $(T_ n), (U_ n)$  be two sequences of r.v., such that:\n",
    "\n",
    "- $\\displaystyle T_ n\\xrightarrow [n\\to \\infty ]{(d)}T$\n",
    "\n",
    "\n",
    "- $\\displaystyle U_ n\\xrightarrow [n\\to \\infty ]{\\mathbf{P}}u$\n",
    "\n",
    "where  $T$  is a r.v. and  $u$  is a given real number (deterministic limit:  $\\mathbf{P}(U=u)=1$ ). Then,\n",
    "\n",
    "- $\\displaystyle {T_ n+U_ n\\xrightarrow [n\\to \\infty ]{(d)}T+u}$\n",
    "\n",
    "\n",
    "- $\\displaystyle {T_ nU_ n\\xrightarrow [n\\to \\infty ]{(d)}Tu}$\n",
    "\n",
    "\n",
    "- If in addition,  $u\\neq 0$ , then $\\displaystyle {\\frac{T_ n}{U_ n}\\xrightarrow [n\\to \\infty ]{(d)}\\frac{T}{u}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"><b>Continuous Mapping Theorem</b> </span>\n",
    "\n",
    "If  $f$  is a continuous function:\n",
    "$$T_ n\\xrightarrow [n\\to \\infty ]{\\mbox{a.s.}/\\mathbf{P}/(d)}T\\hspace{3mm}\\Rightarrow \\hspace{3mm} f(T_ n)\\xrightarrow [n\\to \\infty ]{\\phantom{\\mbox{a.s.}/\\mathbf{P}/(d)}}f(T).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:#6600CC\">Jensen's Inequality </span></h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function  $g:\\mathbb {R}\\to \\mathbb {R}$ is convex if for all pairs of real numbers  $x_1<x_2$\n",
    "\n",
    "$$\\displaystyle  \\displaystyle g(tx_1+(1-t)x_2)\\leq tg(x_1)+(1-t)g(x_2)\\qquad \\text {for all } \\, 0\\leq t\\leq 1.$$\n",
    "                                                                     \n",
    "Geometrically, this means that for  $x_1\\leq x\\leq x_2$, the graph of  g  is below the secant line connecting the two points  $(x_1, g(x_1))$  and  $(x_2,g(x_2))$ .                                               \n",
    "\n",
    "<img src=\"../images/convex.svg\"> </img>\n",
    "\n",
    "\n",
    "At  $x=x_2-t(x_2-x_1)=tx_1+(1-t)x_2$ , the  $y$-value of the graph of  $g$  is  $g(x)=g(tx_1+(1-t)x_2)$ , while the  $y$ -value of the secant line is  $tg(x_1)+(1-t)g(x_2)$ .\n",
    "Note that for  $x_1=0, x_2=1$ , the inequality above can be reinterpretated as follows. Let  $X\\sim \\textsf{Ber}(1-t)$  for some parameter  $0\\leq t \\leq 1$ , then the left and right hand sides of inequality above can be rewritten respectively as:\n",
    "\n",
    "$$\\displaystyle  \\displaystyle g\\left(t(0)+(1-t)(1)\\right) \\quad \\displaystyle = \\quad \\displaystyle g\\left(1-t\\right)\\, \\quad  = \\quad  \\, g\\left(\\mathbb E[X]\\right)$$ \n",
    "\n",
    "\n",
    "$$\\displaystyle t g\\left(0\\right)+(1-t)g\\left(1\\right) \\quad  \\displaystyle = \\quad  \\displaystyle \\mathbb E\\left[g\\left(X\\right)\\right],$$\n",
    "\n",
    "and hence the inequality defining convexity of  $g$  implies\n",
    "\n",
    "$$\\displaystyle  g(\\mathbb E[X])\\leq \\mathbb E[g(X)]\\qquad (\\text {for any Bernoulli random variable} X.)$$\n",
    "\n",
    "\n",
    "**Jensen's Inequality** generalizes this statement to other random variables. It states that for any random variable  $X$ , and any convex function  $g$ ,\n",
    "\n",
    "$$\\displaystyle  \\displaystyle g(\\mathbb E[X])\\leq \\mathbb E[g(X)].$$\n",
    "\n",
    "Jensen's Inequality is also true for random vectors and convex functions on  $\\mathbb {R}^ n$\n",
    "\n",
    "**Memory aid**: To remember which way the inequality goes, remember the special case of the Bernoulli random variable above: the secant line, which is the graph of  $\\mathbb E[g(X)]$ , is above the graph of  g , which is the graph of  $g(\\mathbb E[X])$ .\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and Watch\n",
    "\n",
    "\n",
    "* <a href=\"https://python.quantecon.org/lln_clt.html\"> good simulation for the CLT </a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=0oHjbr2_AhQ\"> CLT proof</a>\n",
    "* <a href=\"https://www.youtube.com/playlist?list=PLwJRxp3blEvZBAn3bwAAtdJqotRPBWBlP\"> An Introduction to the Asymptotic Behaviour of Estimators\n",
    "</a>\n",
    "* <a href=\"https://en.wikipedia.org/wiki/Big_O_in_probability_notation\"> Big O in probability notation\n",
    "</a>\n",
    "* <a href=\"http://www.eecs.harvard.edu/~michaelm/CS222/powerlaw.pdf\"> Power Law</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
